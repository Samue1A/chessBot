{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import chess.engine\n",
    "import random\n",
    "import numpy\n",
    "from stockfish import Stockfish\n",
    "import os\n",
    "import torch\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "\n",
    "def random_board(maxD=200):\n",
    "    board = chess.Board()\n",
    "    depth = random.randrange(0, maxD)\n",
    "    \n",
    "    for _ in range(depth):\n",
    "        all_moves = list(board.legal_moves)\n",
    "        random_move = random.choice(all_moves)\n",
    "        board.push(random_move)\n",
    "        if board.is_game_over():\n",
    "            break\n",
    "    return board\n",
    "\n",
    "def Da_fish(board, depth):\n",
    "    # stockfish = Stockfish('stockfish/stockfish/stockfish-windows-x86-64-avx2.exe')\n",
    "    # stockfish.set_depth(20)\n",
    "    # stockfish.set_skill_level(20)\n",
    "    # stockfish.set_fen_position(board.fen())\n",
    "    # return stockfish.get_evaluation()\n",
    "    with chess.engine.SimpleEngine.popen_uci('stockfish/stockfish/stockfish-windows-x86-64-avx2.exe') as sf:\n",
    "        result = sf.analyse(board, chess.engine.Limit(depth=depth))\n",
    "        score = result['score'].white().score()\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares_index = {\n",
    "    'a':0,\n",
    "    'b':1,\n",
    "    'c':2,\n",
    "    'd':3,\n",
    "    'e':4,\n",
    "    'f':5,\n",
    "    'g':6,\n",
    "    'h':7,\n",
    "}\n",
    "\n",
    "def square_to_index(square):\n",
    "    letter =  chess.square_name(square)\n",
    "    return 8 - int(letter[1]), squares_index[letter[0]]\n",
    "\n",
    "def split_dims(board):\n",
    "    board3d = numpy.zeros((14,8,8), dtype=numpy.int8)\n",
    "    for piece in chess.PIECE_TYPES:\n",
    "        for square in board.pieces(piece, chess.WHITE):\n",
    "            idx = numpy.unravel_index(square, (8, 8))\n",
    "            board3d[piece - 1][7 -idx[0]][idx[1]] = 1\n",
    "        for square in board.pieces(piece, chess.BLACK):\n",
    "            idx = numpy. unravel_index(square, (8, 8))\n",
    "            board3d[piece + 5][7 - idx[0]][idx[1]] = 1\n",
    "    # add attacks and valid moves too\n",
    "    # so the network knows what is being attacked\n",
    "    aux = board.turn\n",
    "    board.turn = chess.WHITE\n",
    "    for move in board.legal_moves:\n",
    "        i, j= square_to_index(move.to_square)\n",
    "        board3d[12][i][j] = 1\n",
    "    board.turn = chess.BLACK\n",
    "    for move in board.legal_moves:\n",
    "        i, j= square_to_index(move.to_square)\n",
    "        board3d[13][i][j] = 1\n",
    "    board.turn = aux\n",
    "    \n",
    "    return board3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6904805 6904805\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def get_dataset():\n",
    "    container = np.load('dataeval/chess_evals.npz')\n",
    "    b, z = container['positions'], container['evaluations']\n",
    "    print(len(b), len(z))\n",
    "    v = np.asarray((np.tanh(3*(z/abs(z).max()))+1)/2)\n",
    "    return b, v, z\n",
    "\n",
    "x_train, y_train, z = get_dataset()\n",
    "\n",
    "x_train = x_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "\n",
    "X_tensor, Y_tensor = torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, conv_size, conv_depth):\n",
    "        super(Net, self).__init__()\n",
    "        # Adjust the in_channels of the first convolutional layer to match the input data\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels=14 if i == 0 else conv_size,\n",
    "                                              out_channels=conv_size,\n",
    "                                              kernel_size=3,\n",
    "                                              padding='same')\n",
    "                                    for i in range(conv_depth)])\n",
    "        self.fc1 = nn.Linear(conv_size * 8 * 8, 64)  # Adjust the size accordingly if the input volume changes\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply the convolutional layers\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Adjust `conv_size` and `conv_depth` as needed\n",
    "conv_size = 64\n",
    "conv_depth = 5\n",
    "net = Net(conv_size, conv_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += residual\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "    \n",
    "class ResNetChess(nn.Module):\n",
    "    def __init__(self, conv_size, conv_depth):\n",
    "        super(ResNetChess, self).__init__()\n",
    "        self.initial_conv = nn.Conv2d(in_channels=14, out_channels=conv_size, kernel_size=3, padding='same')\n",
    "        self.res_blocks = nn.Sequential(*[ResidualBlock(conv_size) for _ in range(conv_depth)])\n",
    "        self.fc1 = nn.Linear(conv_size * 8 * 8, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.initial_conv(x))\n",
    "        x = self.res_blocks(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "conv_size = 64\n",
    "conv_depth = 5\n",
    "net = ResNetChess(conv_size, conv_depth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 500: 692.1298503875732\n",
      "loss at step 1000: 692.2876238822937\n",
      "loss at step 1500: 691.4325952529907\n",
      "loss at step 2000: 691.941499710083\n",
      "loss at step 2500: 690.6639337539673\n",
      "loss at step 3000: 690.5879974365234\n",
      "loss at step 3500: 690.5093193054199\n",
      "loss at step 4000: 690.9477710723877\n",
      "loss at step 4500: 691.0675764083862\n",
      "loss at step 5000: 692.0304298400879\n",
      "loss at step 5500: 692.315936088562\n",
      "loss at step 6000: 691.8141841888428\n",
      "loss at step 6500: 689.4163489341736\n",
      "loss at step 7000: 689.3925070762634\n",
      "loss at step 7500: 689.8553371429443\n",
      "loss at step 8000: 689.3571615219116\n",
      "loss at step 8500: 690.1525855064392\n",
      "loss at step 9000: 688.3330345153809\n",
      "loss at step 9500: 690.5763149261475\n",
      "loss at step 10000: 690.6561255455017\n",
      "loss at step 10500: 690.8711194992065\n",
      "loss at step 11000: 692.8887367248535\n",
      "loss at step 11500: 690.293550491333\n",
      "loss at step 12000: 688.2592439651489\n",
      "loss at step 12500: 689.7434592247009\n",
      "loss at step 13000: 688.8603568077087\n",
      "Epoch 1, Loss: 690.9198578274181\n",
      "loss at step 500: 690.7730102539062\n",
      "loss at step 1000: 690.857470035553\n",
      "loss at step 1500: 691.455066204071\n",
      "loss at step 2000: 690.1630759239197\n",
      "loss at step 2500: 692.0236349105835\n",
      "loss at step 3000: 690.3770565986633\n",
      "loss at step 3500: 690.024733543396\n",
      "loss at step 4000: 689.433217048645\n",
      "loss at step 4500: 690.1341676712036\n",
      "loss at step 5000: 690.1749968528748\n",
      "loss at step 5500: 690.7684803009033\n",
      "loss at step 6000: 690.8004879951477\n",
      "loss at step 6500: 691.2238001823425\n",
      "loss at step 7000: 689.1956925392151\n",
      "loss at step 7500: 688.3262395858765\n",
      "loss at step 8000: 690.9017562866211\n",
      "loss at step 8500: 688.9224648475647\n",
      "loss at step 9000: 689.5865201950073\n",
      "loss at step 9500: 689.6265745162964\n",
      "loss at step 10000: 691.447913646698\n",
      "loss at step 10500: 689.7755861282349\n",
      "loss at step 11000: 689.4149780273438\n",
      "loss at step 11500: 690.0748014450073\n",
      "loss at step 12000: 689.9186372756958\n",
      "loss at step 12500: 690.8950805664062\n",
      "loss at step 13000: 689.4111633300781\n",
      "Epoch 2, Loss: 689.6511966045755\n",
      "loss at step 500: 688.0604028701782\n",
      "loss at step 1000: 686.5326166152954\n",
      "loss at step 1500: 691.2107467651367\n",
      "loss at step 2000: 688.5350346565247\n",
      "loss at step 2500: 690.7966732978821\n",
      "loss at step 3000: 689.9963617324829\n",
      "loss at step 3500: 691.749095916748\n",
      "loss at step 4000: 688.9631748199463\n",
      "loss at step 4500: 689.7066831588745\n",
      "loss at step 5000: 691.4401054382324\n",
      "loss at step 5500: 689.8304224014282\n",
      "loss at step 6000: 690.3150677680969\n",
      "loss at step 6500: 691.2969350814819\n",
      "loss at step 7000: 690.7909512519836\n",
      "loss at step 7500: 687.1375441551208\n",
      "loss at step 8000: 690.9875869750977\n",
      "loss at step 8500: 690.922737121582\n",
      "loss at step 9000: 687.4552369117737\n",
      "loss at step 9500: 691.0068988800049\n",
      "loss at step 10000: 688.3434653282166\n",
      "loss at step 10500: 690.3891563415527\n",
      "loss at step 11000: 689.8388862609863\n",
      "loss at step 11500: 687.1721744537354\n",
      "loss at step 12000: 687.9250407218933\n",
      "loss at step 12500: 689.1731023788452\n",
      "loss at step 13000: 690.9265518188477\n",
      "Epoch 3, Loss: 689.3235413598215\n",
      "loss at step 500: 688.533365726471\n",
      "loss at step 1000: 687.0474219322205\n",
      "loss at step 1500: 689.4165277481079\n",
      "loss at step 2000: 689.100980758667\n",
      "loss at step 2500: 687.8398656845093\n",
      "loss at step 3000: 687.8082752227783\n",
      "loss at step 3500: 689.8561716079712\n",
      "loss at step 4000: 690.0457739830017\n",
      "loss at step 4500: 688.4354948997498\n",
      "loss at step 5000: 689.071536064148\n",
      "loss at step 5500: 688.513994216919\n",
      "loss at step 6000: 689.394474029541\n",
      "loss at step 6500: 690.014898777008\n",
      "loss at step 7000: 689.033031463623\n",
      "loss at step 7500: 689.6113753318787\n",
      "loss at step 8000: 688.2113814353943\n",
      "loss at step 8500: 690.1019215583801\n",
      "loss at step 9000: 688.3798837661743\n",
      "loss at step 9500: 688.0024075508118\n",
      "loss at step 10000: 688.0873441696167\n",
      "loss at step 10500: 690.9763813018799\n",
      "loss at step 11000: 691.540002822876\n",
      "loss at step 11500: 690.1274919509888\n",
      "loss at step 12000: 691.5497779846191\n",
      "loss at step 12500: 689.782977104187\n",
      "loss at step 13000: 688.4310245513916\n",
      "Epoch 4, Loss: 689.1161765089449\n",
      "loss at step 500: 688.0034804344177\n",
      "loss at step 1000: 689.6423697471619\n",
      "loss at step 1500: 688.3947849273682\n",
      "loss at step 2000: 690.0273561477661\n",
      "loss at step 2500: 687.72292137146\n",
      "loss at step 3000: 689.8072361946106\n",
      "loss at step 3500: 689.4480586051941\n",
      "loss at step 4000: 687.6242160797119\n",
      "loss at step 4500: 689.4440650939941\n",
      "loss at step 5000: 687.6277923583984\n",
      "loss at step 5500: 688.2866621017456\n",
      "loss at step 6000: 689.7193193435669\n",
      "loss at step 6500: 684.1307282447815\n",
      "loss at step 7000: 690.536618232727\n",
      "loss at step 7500: 689.1379952430725\n",
      "loss at step 8000: 689.3386244773865\n",
      "loss at step 8500: 689.25940990448\n",
      "loss at step 9000: 689.733624458313\n",
      "loss at step 9500: 685.120165348053\n",
      "loss at step 10000: 690.254807472229\n",
      "loss at step 10500: 689.4795298576355\n",
      "loss at step 11000: 689.5266771316528\n",
      "loss at step 11500: 686.7040395736694\n",
      "loss at step 12000: 689.9628639221191\n",
      "loss at step 12500: 688.4033679962158\n",
      "loss at step 13000: 688.0341172218323\n",
      "Epoch 5, Loss: 688.957096556077\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Assuming your code and the Net class definition are here\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)\n",
    "\n",
    "# Initial learning rate\n",
    "lr_initial = 0.001\n",
    "# Set the optimizer with the initial learning rate\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr_initial)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5  # Adjust the number of epochs as needed\n",
    "\n",
    "# Calculate gamma for exponential decay\n",
    "lr_final = 0.0005\n",
    "gamma = (lr_final / lr_initial) ** (1 / num_epochs)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    ssss = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        ssss += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to device in batches\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        outputs = torch.squeeze(outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if ssss%500 == 0:\n",
    "        \n",
    "            print(f\"loss at step {ssss}: {loss.item()*1000}\")\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Decay the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(dataloader)*1000}')\n",
    "\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetChess(\n",
       "  (initial_conv): Conv2d(14, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "  (res_blocks): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=4096, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.save(net.state_dict(), 'model_weights.pth')\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6926039990938755\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Assuming your code and the Net class definition are here\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "# Create a dataset. Note: Do not move the entire dataset to the device here\n",
    "dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1  # Adjust the number of epochs as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to device in batches\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        outputs = torch.squeeze(outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(dataloader)}')\n",
    "    \n",
    "torch.save(net.state_dict(), 'model_weights.pth')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6904805 6904805\n",
      "Epoch 1, Loss: 0.692553937472718\n",
      "Epoch 2, Loss: 0.6925201222784492\n",
      "Epoch 3, Loss: 0.6924936466065974\n",
      "Epoch 4, Loss: 0.6924719352070168\n",
      "Epoch 5, Loss: 0.6924518246049522\n",
      "Epoch 6, Loss: 0.6924331563659013\n",
      "Epoch 7, Loss: 0.6924164594976605\n",
      "Epoch 8, Loss: 0.6924000959199772\n",
      "Epoch 9, Loss: 0.6923837563459144\n",
      "Epoch 10, Loss: 0.6923694579845346\n",
      "2345781 2345781\n",
      "Epoch 1, Loss: 0.6922634187765647\n",
      "Epoch 2, Loss: 0.6920715767473312\n",
      "Epoch 3, Loss: 0.6919812778602575\n",
      "Epoch 4, Loss: 0.6919121772624885\n",
      "Epoch 5, Loss: 0.6918553032142386\n",
      "Epoch 6, Loss: 0.6918049996487344\n",
      "Epoch 7, Loss: 0.6917563942318209\n",
      "Epoch 8, Loss: 0.691711490267576\n",
      "Epoch 9, Loss: 0.6916697705742341\n",
      "Epoch 10, Loss: 0.6916302678959325\n",
      "2345781 2345781\n",
      "Epoch 1, Loss: 0.6915937741057614\n",
      "Epoch 2, Loss: 0.6915629702516695\n",
      "Epoch 3, Loss: 0.6915323496606389\n",
      "Epoch 4, Loss: 0.6915005252554743\n",
      "Epoch 5, Loss: 0.6914759125927827\n",
      "Epoch 6, Loss: 0.691449770113317\n",
      "Epoch 7, Loss: 0.6914301365898512\n",
      "Epoch 8, Loss: 0.6914138218030514\n",
      "Epoch 9, Loss: 0.6913917395294139\n",
      "Epoch 10, Loss: 0.6913785754987103\n",
      "[array([-10, -56,  -9, ..., -13,   0,   0]), array([-408, -444,  428, ...,  944,  692, 1138]), array([-408, -444,  428, ...,  944,  692, 1138])]\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "def get_dataset(doc):\n",
    "    container = np.load(doc)\n",
    "    b, z = container['positions'], container['evaluations']\n",
    "    print(len(b), len(z))\n",
    "    v = np.asarray(z/abs(z).max()/2+0.5)\n",
    "    return b, v, z\n",
    "\n",
    "docs = [\n",
    "    'dataeval/chess_evals.npz',\n",
    "    'dataeval/random_evals.npz',\n",
    "    'dataeval/tactics_evals.npz'\n",
    "]\n",
    "\n",
    "allZS = []\n",
    "\n",
    "for doc in docs:\n",
    "    x_train, y_train, z = get_dataset(doc)\n",
    "    \n",
    "    allZS.append(z)\n",
    "\n",
    "    x_train = x_train.astype(np.float32)\n",
    "    y_train = y_train.astype(np.float32)\n",
    "\n",
    "    X_tensor, Y_tensor = torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "    # Assuming your code and the Net class definition are here\n",
    "\n",
    "    # Set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "    # Create a dataset. Note: Do not move the entire dataset to the device here\n",
    "    dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 10  # Adjust the number of epochs as needed\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to device in batches\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            outputs = torch.squeeze(outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(dataloader)}')\n",
    "    \n",
    "# torch.save(net.state_dict(), 'model_weights.pth')\n",
    "print(allZS)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ResNetChess.forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m board \u001b[38;5;241m=\u001b[39m random_board()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming you have the model architecture defined\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m  \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconv_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_depth\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# the model definition must be exactly the same as the saved model\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load the model state\u001b[39;00m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_weights.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\samue\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\samue\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: ResNetChess.forward() takes 2 positional arguments but 3 were given"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "board = random_board()\n",
    "\n",
    "# Assuming you have the model architecture defined\n",
    "model =  net(conv_size, conv_depth) # the model definition must be exactly the same as the saved model\n",
    "\n",
    "# Load the model state\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Don't forget to switch to eval mode if you're doing inference\n",
    "model.eval()\n",
    "\n",
    "\n",
    "print(model(torch.tensor(numpy.expand_dims(split_dims(board), 0), dtype=torch.float32).to(device)))\n",
    "print(Da_fish(board, 10)/2000)\n",
    "print(np.asarray((np.tanh(3*(Da_fish(board, 10)/abs(z).max()))+1)/2))\n",
    "board"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
